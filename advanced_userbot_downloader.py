#!/usr/bin/env python3
# advanced_userbot_downloader.py - UserBot Ø§ÛŒÙ…Ù† Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ

from telethon import TelegramClient, events, types
from telethon.tl.types import MessageMediaDocument, MessageMediaPhoto, DocumentAttributeFilename
import asyncio
import os
import json
import random
import logging
import re
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
import aiohttp
from dataclasses import dataclass, asdict
from enum import Enum
import sqlite3
from contextlib import contextmanager
import pickle
from concurrent.futures import ThreadPoolExecutor

# ==================== Ø±ÙØ¹ Ø§ÛŒØ±Ø§Ø¯Ø§Øª Ø§ØµÙ„ÛŒ ====================

# 1. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† try-except Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø¹Ù…Ù„ÛŒØ§Øª I/O
# 2. Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø¯ÛŒØ±ÛŒØª session
# 3. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timeout Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§Øª Ø´Ø¨Ú©Ù‡
# 4. Ø±ÙØ¹ Ù…Ø´Ú©Ù„ duplicate downloads
# 5. Ø¨Ù‡Ø¨ÙˆØ¯ progress reporting

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================

class ColoredFormatter(logging.Formatter):
    """ÙØ±Ù…ØªØ± Ø±Ù†Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§"""
    COLORS = {
        'DEBUG': '\033[94m',      # Ø¢Ø¨ÛŒ
        'INFO': '\033[92m',       # Ø³Ø¨Ø²
        'WARNING': '\033[93m',    # Ø²Ø±Ø¯
        'ERROR': '\033[91m',      # Ù‚Ø±Ù…Ø²
        'CRITICAL': '\033[91m\033[1m',  # Ù‚Ø±Ù…Ø² Ù¾Ø±Ø±Ù†Ú¯
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        message = super().format(record)
        return f"{log_color}{message}{self.COLORS['RESET']}"

def setup_logging():
    """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # ÙØ±Ù…Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
    formatter = ColoredFormatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Handler ÙØ§ÛŒÙ„
    file_handler = logging.FileHandler('userbot_advanced.log', encoding='utf-8', mode='a')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    
    # Handler Ú©Ù†Ø³ÙˆÙ„
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# ==================== Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ====================

class AIContentAnalyzer:
    """Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
    
    class ContentType(Enum):
        TEXT = "text"
        IMAGE = "image"
        VIDEO = "video"
        AUDIO = "audio"
        DOCUMENT = "document"
        UNKNOWN = "unknown"
    
    @dataclass
    class ContentAnalysis:
        content_type: str
        confidence: float
        tags: List[str]
        category: str
        nsfw_score: float
        summary: Optional[str]
        language: str
        keywords: List[str]
        sentiment: str
        file_hash: str
        quality_score: float
        
    def __init__(self):
        self.initialized = False
        self.cache = {}
        self.executor = ThreadPoolExecutor(max_workers=2)
        
    async def initialize(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI (Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø¨Ú©)"""
        try:
            # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ AI Ø±Ø§ Ù„ÙˆØ¯ Ú©Ø±Ø¯
            # Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ù†Ø·Ù‚ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            logger.info("âœ… AI Analyzer initialized (lightweight mode)")
            self.initialized = True
            
            # Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
            self.keyword_categories = {
                'educational': ['Ø¢Ù…ÙˆØ²Ø´', 'Ø¯Ø±Ø³', 'Ú©ØªØ§Ø¨', 'ØªØ­ØµÛŒÙ„', 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', 'Ù…Ø¯Ø±Ø³Ù‡'],
                'entertainment': ['ÙÛŒÙ„Ù…', 'Ø³Ø±ÛŒØ§Ù„', 'Ú©Ø§Ø±ØªÙˆÙ†', 'Ù…ÙˆØ³ÛŒÙ‚ÛŒ', 'Ø·Ù†Ø²', 'ØªÙØ±ÛŒØ­'],
                'technology': ['Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ú©Ø¯', 'Ù¾Ø§ÛŒØªÙˆÙ†', 'Ù‡ÙˆØ´', 'Ù…ØµÙ†ÙˆØ¹ÛŒ', 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±'],
                'news': ['Ø§Ø®Ø¨Ø§Ø±', 'Ø³ÛŒØ§Ø³ÛŒ', 'Ø§Ù‚ØªØµØ§Ø¯', 'Ø­ÙˆØ§Ø¯Ø«', 'ÙˆØ±Ø²Ø´'],
                'religious': ['Ù…Ø°Ù‡Ø¨ÛŒ', 'Ù‚Ø±Ø¢Ù†', 'Ø§Ø°Ø§Ù†', 'Ø¯Ø¹Ø§', 'Ø±ÙˆØ¶Ù‡']
            }
            
            self.nsfw_keywords = ['Ù…Ù…Ù†ÙˆØ¹', 'Ø³Ú©Ø³ÛŒ', 'Ø¬Ù†Ø³ÛŒ', 'Ù…Ø­Ø±Ù…Ø§Ù†Ù‡', 'Ø®ØµÙˆØµÛŒ']
            
        except Exception as e:
            logger.warning(f"AI initialization failed, using fallback: {e}")
            self.initialized = False
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø´ ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªÚ©Ø±Ø§Ø±ÛŒ"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read(8192)).hexdigest()  # ÙÙ‚Ø· Ø¨Ø®Ø´ Ø§ÙˆÙ„ ÙØ§ÛŒÙ„
        except:
            return ""
    
    async def analyze_text(self, text: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ† Ø¨Ø§ AI Ø³Ø§Ø¯Ù‡"""
        if not text:
            return {'category': 'unknown', 'sentiment': 'neutral'}
        
        text_lower = text.lower()
        
        # ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        category = 'other'
        max_matches = 0
        for cat, keywords in self.keyword_categories.items():
            matches = sum(1 for kw in keywords if kw in text_lower)
            if matches > max_matches:
                max_matches = matches
                category = cat
        
        # ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø³Ø§Ø¯Ù‡
        positive_words = ['Ø®ÙˆØ¨', 'Ø¹Ø§Ù„ÛŒ', 'Ù…Ù…ØªØ§Ø²', 'Ø¹Ø§Ù„ÛŒ', 'Ø²ÛŒØ¨Ø§', 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†ÛŒ']
        negative_words = ['Ø¨Ø¯', 'Ø¶Ø¹ÛŒÙ', 'Ø®Ø±Ø§Ø¨', 'Ù…Ø´Ú©Ù„', 'Ø®Ø·Ø§', 'Ø§Ø´ØªØ¨Ø§Ù‡']
        
        pos_count = sum(1 for w in positive_words if w in text_lower)
        neg_count = sum(1 for w in negative_words if w in text_lower)
        
        sentiment = 'neutral'
        if pos_count > neg_count:
            sentiment = 'positive'
        elif neg_count > pos_count:
            sentiment = 'negative'
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        words = re.findall(r'\w{3,}', text_lower)
        word_freq = {}
        for word in words:
            if word not in ['Ø¨Ø±Ø§ÛŒ', 'Ù‡Ø§ÛŒ', 'Ø§Ø³Øª', 'Ø§ÛŒÙ†', 'Ú©Ù‡']:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        keywords = [k[0] for k in keywords]
        
        # ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† (Ø³Ø§Ø¯Ù‡)
        lang = 'fa' if re.search(r'[\u0600-\u06FF]', text) else 'en'
        
        # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ (Ø§ÙˆÙ„ÛŒÙ† Ø¬Ù…Ù„Ù‡)
        sentences = re.split(r'[.!?]', text)
        summary = sentences[0][:100] + '...' if sentences else ""
        
        # ØªØ´Ø®ÛŒØµ NSFW
        nsfw_score = 0.0
        for kw in self.nsfw_keywords:
            if kw in text_lower:
                nsfw_score += 0.2
        
        return {
            'category': category,
            'sentiment': sentiment,
            'keywords': keywords,
            'language': lang,
            'summary': summary,
            'nsfw_score': min(nsfw_score, 1.0),
            'text_length': len(text)
        }
    
    async def analyze_file(self, file_path: Path, file_type: str, caption: str = "") -> ContentAnalysis:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ ÙØ§ÛŒÙ„"""
        try:
            # ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ† Ú©Ù¾Ø´Ù†
            text_analysis = await self.analyze_text(caption)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø´ ÙØ§ÛŒÙ„
            file_hash = self.calculate_file_hash(file_path)
            
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ ÙØ§ÛŒÙ„ Ø§Ø² Ù¾Ø³ÙˆÙ†Ø¯
            ext = file_path.suffix.lower()
            content_type = self.ContentType.DOCUMENT.value
            
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                content_type = self.ContentType.IMAGE.value
                quality_score = await self.estimate_image_quality(file_path)
            elif ext in ['.mp4', '.avi', '.mov', '.mkv']:
                content_type = self.ContentType.VIDEO.value
                quality_score = 0.7
            elif ext in ['.mp3', '.wav', '.ogg', '.flac']:
                content_type = self.ContentType.AUDIO.value
                quality_score = 0.6
            else:
                quality_score = 0.5
            
            # ØªÙˆÙ„ÛŒØ¯ ØªÚ¯â€ŒÙ‡Ø§
            tags = []
            tags.append(content_type)
            tags.append(text_analysis['category'])
            tags.extend(text_analysis['keywords'][:3])
            
            # Ø³Ø§Ø®Øª ØªØ­Ù„ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ
            analysis = self.ContentAnalysis(
                content_type=content_type,
                confidence=0.8,
                tags=tags,
                category=text_analysis['category'],
                nsfw_score=text_analysis['nsfw_score'],
                summary=text_analysis['summary'],
                language=text_analysis['language'],
                keywords=text_analysis['keywords'],
                sentiment=text_analysis['sentiment'],
                file_hash=file_hash,
                quality_score=quality_score
            )
            
            logger.info(f"ğŸ§  AI Analysis: {file_path.name} -> {analysis.category} ({analysis.sentiment})")
            return analysis
            
        except Exception as e:
            logger.error(f"AI analysis failed: {e}")
            # Ø¨Ø§Ø²Ú¯Ø´Øª ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            return self.ContentAnalysis(
                content_type=file_type,
                confidence=0.5,
                tags=[file_type],
                category='unknown',
                nsfw_score=0.0,
                summary=None,
                language='unknown',
                keywords=[],
                sentiment='neutral',
                file_hash='',
                quality_score=0.5
            )
    
    async def estimate_image_quality(self, image_path: Path) -> float:
        """ØªØ®Ù…ÛŒÙ† Ú©ÛŒÙÛŒØª ØªØµÙˆÛŒØ± (Ø³Ø§Ø¯Ù‡)"""
        try:
            import PIL.Image as PILImage
            with PILImage.open(image_path) as img:
                # Ø¹ÙˆØ§Ù…Ù„ Ú©ÛŒÙÛŒØª Ø³Ø§Ø¯Ù‡
                score = 0.5
                
                # Ø§Ù†Ø¯Ø§Ø²Ù‡
                width, height = img.size
                if width > 1000 and height > 1000:
                    score += 0.2
                elif width < 300 or height < 300:
                    score -= 0.2
                
                # ÙØ±Ù…Øª
                if img.format in ['JPEG', 'PNG']:
                    score += 0.1
                
                return max(0.1, min(1.0, score))
        except:
            return 0.5
    
    async def is_duplicate(self, file_path: Path, known_hashes: Set[str]) -> bool:
        """ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„ ØªÚ©Ø±Ø§Ø±ÛŒ"""
        if not file_path.exists():
            return False
        
        file_hash = self.calculate_file_hash(file_path)
        if not file_hash:
            return False
        
        return file_hash in known_hashes
    
    async def filter_content(self, analysis: ContentAnalysis, user_preferences: Dict) -> bool:
        """ÙÛŒÙ„ØªØ± Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ø§Ø±Ø¨Ø±"""
        try:
            # ÙÛŒÙ„ØªØ± NSFW
            if user_preferences.get('block_nsfw', True) and analysis.nsfw_score > 0.7:
                logger.info(f"âš ï¸ Blocked NSFW content: {analysis.nsfw_score}")
                return False
            
            # ÙÛŒÙ„ØªØ± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
            blocked_categories = user_preferences.get('blocked_categories', [])
            if analysis.category in blocked_categories:
                logger.info(f"âš ï¸ Blocked category: {analysis.category}")
                return False
            
            # ÙÛŒÙ„ØªØ± Ú©ÛŒÙÛŒØª
            min_quality = user_preferences.get('min_quality', 0.3)
            if analysis.quality_score < min_quality:
                logger.info(f"âš ï¸ Low quality: {analysis.quality_score}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Filter error: {e}")
            return True  # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ù…Ø­ØªÙˆØ§ Ø±Ø§ Ø±Ø¯ Ù†Ú©Ù†

# ==================== Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================

class DatabaseManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ SQLite Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Ø¬Ø¯ÙˆÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_hash TEXT UNIQUE,
                telegram_id INTEGER,
                chat_id INTEGER,
                chat_title TEXT,
                file_name TEXT,
                file_path TEXT,
                file_size INTEGER,
                file_type TEXT,
                download_time TEXT,
                caption TEXT,
                category TEXT,
                tags TEXT,
                language TEXT,
                sentiment TEXT,
                nsfw_score REAL,
                quality_score REAL,
                ai_summary TEXT,
                is_processed BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS channels (
                id INTEGER PRIMARY KEY,
                username TEXT,
                title TEXT,
                added_date TEXT,
                last_check TEXT,
                is_active BOOLEAN DEFAULT 1,
                category TEXT,
                priority INTEGER DEFAULT 5,
                daily_limit INTEGER DEFAULT 20,
                total_downloads INTEGER DEFAULT 0
            )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø¢Ù…Ø§Ø±
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS statistics (
                date TEXT PRIMARY KEY,
                downloads INTEGER DEFAULT 0,
                errors INTEGER DEFAULT 0,
                total_size INTEGER DEFAULT 0,
                avg_quality REAL DEFAULT 0,
                categories TEXT
            )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ú©Ø´ Ù‡Ø´ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS file_hashes (
                file_hash TEXT PRIMARY KEY,
                file_path TEXT,
                file_size INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            ''')
            
            conn.commit()
    
    @contextmanager
    def get_connection(self):
        """Ù…Ø¯ÛŒØ±ÛŒØª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        conn = sqlite3.connect(self.db_path, timeout=10)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()
    
    def save_file_info(self, file_info: Dict, ai_analysis: AIContentAnalyzer.ContentAnalysis = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù‡Ø´ ØªÚ©Ø±Ø§Ø±ÛŒ
                if ai_analysis and ai_analysis.file_hash:
                    cursor.execute(
                        "SELECT id FROM files WHERE file_hash = ?",
                        (ai_analysis.file_hash,)
                    )
                    if cursor.fetchone():
                        logger.warning(f"Duplicate file detected: {ai_analysis.file_hash}")
                        return False
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ file_hashes
                if ai_analysis and ai_analysis.file_hash:
                    cursor.execute('''
                    INSERT OR IGNORE INTO file_hashes (file_hash, file_path, file_size)
                    VALUES (?, ?, ?)
                    ''', (ai_analysis.file_hash, file_info.get('file_path'), file_info.get('file_size', 0)))
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ files
                cursor.execute('''
                INSERT INTO files (
                    file_hash, telegram_id, chat_id, chat_title, file_name,
                    file_path, file_size, file_type, download_time, caption,
                    category, tags, language, sentiment, nsfw_score,
                    quality_score, ai_summary, is_processed
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    ai_analysis.file_hash if ai_analysis else '',
                    file_info.get('id'),
                    file_info.get('chat_id'),
                    file_info.get('chat_title', 'Unknown'),
                    file_info.get('file_name'),
                    file_info.get('file_path'),
                    file_info.get('file_size', 0),
                    file_info.get('file_type', ''),
                    file_info.get('download_time'),
                    file_info.get('caption', '')[:500],  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„
                    ai_analysis.category if ai_analysis else 'unknown',
                    ','.join(ai_analysis.tags) if ai_analysis else '',
                    ai_analysis.language if ai_analysis else 'unknown',
                    ai_analysis.sentiment if ai_analysis else 'neutral',
                    ai_analysis.nsfw_score if ai_analysis else 0.0,
                    ai_analysis.quality_score if ai_analysis else 0.5,
                    ai_analysis.summary if ai_analysis else '',
                    0
                ))
                
                conn.commit()
                logger.info(f"ğŸ’¾ File info saved to database: {file_info.get('file_name')}")
                return True
                
        except sqlite3.Error as e:
            logger.error(f"Database error: {e}")
            return False
    
    def get_duplicate_hashes(self) -> Set[str]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Ù‡Ø´â€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        hashes = set()
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT file_hash FROM file_hashes WHERE file_hash != ''")
                rows = cursor.fetchall()
                hashes = {row['file_hash'] for row in rows}
        except Exception as e:
            logger.error(f"Error getting hashes: {e}")
        return hashes
    
    def update_statistics(self, file_size: int, category: str):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯ Ø§Ù…Ø±ÙˆØ²
                cursor.execute(
                    "SELECT downloads, total_size, categories FROM statistics WHERE date = ?",
                    (today,)
                )
                row = cursor.fetchone()
                
                if row:
                    downloads = row['downloads'] + 1
                    total_size = row['total_size'] + file_size
                    
                    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§
                    categories = json.loads(row['categories']) if row['categories'] else {}
                    categories[category] = categories.get(category, 0) + 1
                    
                    cursor.execute('''
                    UPDATE statistics 
                    SET downloads = ?, total_size = ?, categories = ?
                    WHERE date = ?
                    ''', (downloads, total_size, json.dumps(categories), today))
                else:
                    cursor.execute('''
                    INSERT INTO statistics (date, downloads, total_size, categories)
                    VALUES (?, 1, ?, ?)
                    ''', (today, file_size, json.dumps({category: 1})))
                
                conn.commit()
                
        except Exception as e:
            logger.error(f"Statistics update error: {e}")

# ==================== UserBot Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================

class AdvancedUserBotDownloader:
    """UserBot Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    
    def __init__(self, api_id: int, api_hash: str):
        self.api_id = api_id
        self.api_hash = api_hash
        self.client = None
        
        # Ù…Ø³ÛŒØ±Ù‡Ø§
        self.base_dir = Path(__file__).parent
        self.downloads_dir = self.base_dir / "downloads"
        self.data_dir = self.base_dir / "data"
        self.db_path = self.data_dir / "userbot.db"
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
        for directory in [self.downloads_dir, self.data_dir]:
            directory.mkdir(exist_ok=True)
        
        # Ù…Ø¯ÛŒØ±Ø§Ù† Ø³ÛŒØ³ØªÙ…
        self.db = DatabaseManager(self.db_path)
        self.ai_analyzer = AIContentAnalyzer()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.settings = {
            'safety': {
                'max_downloads_per_day': 30,
                'min_delay': 2.0,
                'max_delay': 8.0,
                'cooldown_after_error': 60,
                'working_hours': [(9, 13), (16, 23)],
                'skip_weekends': True,
                'max_file_size_mb': 500,  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø­Ø¬Ù… ÙØ§ÛŒÙ„
            },
            'ai': {
                'enable_analysis': True,
                'filter_nsfw': True,
                'min_quality_score': 0.3,
                'blocked_categories': ['adult', 'spam'],
                'auto_organize': True,
            },
            'organization': {
                'categorize_by_type': True,
                'categorize_by_content': True,
                'create_date_folders': True,
                'rename_files': False,
            }
        }
        
        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø­Ø§Ù„Øª
        self.download_count_today = 0
        self.last_reset_date = datetime.now().date()
        self.known_hashes = self.db.get_duplicate_hashes()
        
        logger.info("ğŸš€ Advanced UserBot Downloader Initialized")
    
    async def initialize_ai(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³ÛŒØ³ØªÙ… AI"""
        await self.ai_analyzer.initialize()
        logger.info("âœ… AI System Ready")
    
    async def human_delay(self, min_sec: float = None, max_sec: float = None):
        """ØªØ§Ø®ÛŒØ± Ø§Ù†Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        if min_sec is None:
            min_sec = self.settings['safety']['min_delay']
        if max_sec is None:
            max_sec = self.settings['safety']['max_delay']
        
        # ØªØ§Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ Ø¨Ø§ ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„
        base_delay = random.uniform(min_sec, max_sec)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØºÛŒÛŒØ±Ø§Øª Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒâ€ŒØªØ± Ø´Ø¯Ù†
        jitter = random.uniform(-0.5, 0.5)
        total_delay = max(0.5, base_delay + jitter)
        
        logger.debug(f"â³ Human delay: {total_delay:.2f}s")
        await asyncio.sleep(total_delay)
    
    def is_safe_to_operate(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø§ÛŒØ· Ø§ÛŒÙ…Ù† Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        now = datetime.now()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø®Ø± Ù‡ÙØªÙ‡
        if self.settings['safety']['skip_weekends'] and now.weekday() >= 5:
            logger.info("ğŸŒ Weekend - Operation paused")
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø¹Øª Ú©Ø§Ø±ÛŒ
        current_hour = now.hour
        working_hours = self.settings['safety']['working_hours']
        
        for start, end in working_hours:
            if start <= current_hour < end:
                return True
        
        logger.info(f"â° Outside working hours ({current_hour}:00)")
        return False
    
    def can_download_more(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ú©Ø§Ù† Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨ÛŒØ´ØªØ±"""
        # Ø¨Ø±Ø±Ø³ÛŒ Ø±ÛŒØ³Øª Ø±ÙˆØ²Ø§Ù†Ù‡
        today = datetime.now().date()
        if today != self.last_reset_date:
            self.last_reset_date = today
            self.download_count_today = 0
            logger.info("ğŸ”„ Daily counter reset")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡
        daily_limit = self.settings['safety']['max_downloads_per_day']
        if self.download_count_today >= daily_limit:
            logger.warning(f"ğŸš« Daily limit reached: {self.download_count_today}/{daily_limit}")
            return False
        
        return True
    
    async def simulate_human_activity(self, chat_id):
        """Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ÙØ¹Ø§Ù„ÛŒØª Ø§Ù†Ø³Ø§Ù†ÛŒ"""
        activities = [
            ('typing', 1.0, 3.0),
            ('upload_photo', 0.5, 2.0),
            ('record_video', 1.5, 4.0),
        ]
        
        activity, min_time, max_time = random.choice(activities)
        
        try:
            async with self.client.action(chat_id, activity):
                duration = random.uniform(min_time, max_time)
                await asyncio.sleep(duration)
                logger.debug(f"ğŸ‘¤ Simulated {activity} for {duration:.1f}s")
        except Exception as e:
            logger.debug(f"Activity simulation failed: {e}")
    
    async def get_file_name(self, message) -> Tuple[str, str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ùˆ Ù¾Ø³ÙˆÙ†Ø¯ ÙØ§ÛŒÙ„ Ø§Ø² Ù¾ÛŒØ§Ù…"""
        try:
            if hasattr(message, 'document') and message.document:
                for attr in message.document.attributes:
                    if isinstance(attr, DocumentAttributeFilename):
                        filename = attr.file_name
                        ext = Path(filename).suffix.lower()
                        return filename, ext
            
            elif hasattr(message, 'video') and message.video:
                return f"video_{message.id}.mp4", '.mp4'
            
            elif hasattr(message, 'audio') and message.audio:
                return f"audio_{message.id}.mp3", '.mp3'
            
            elif hasattr(message, 'photo') and message.photo:
                return f"photo_{message.id}.jpg", '.jpg'
            
            elif hasattr(message, 'voice') and message.voice:
                return f"voice_{message.id}.ogg", '.ogg'
            
        except Exception as e:
            logger.error(f"Error getting filename: {e}")
        
        return f"file_{message.id}.bin", '.bin'
    
    async def organize_file(self, file_path: Path, analysis: AIContentAnalyzer.ContentAnalysis) -> Path:
        """Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§ÛŒÙ„ Ø¯Ø± Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨"""
        if not self.settings['organization']['auto_organize']:
            return file_path
        
        try:
            # Ø³Ø§Ø®ØªØ§Ø± Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
            parts = []
            
            if self.settings['organization']['create_date_folders']:
                date_folder = datetime.now().strftime('%Y-%m-%d')
                parts.append(date_folder)
            
            if self.settings['organization']['categorize_by_type']:
                parts.append(analysis.content_type)
            
            if self.settings['organization']['categorize_by_content']:
                parts.append(analysis.category)
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
            if parts:
                new_dir = self.downloads_dir / Path(*parts)
                new_dir.mkdir(parents=True, exist_ok=True)
                
                new_path = new_dir / file_path.name
                
                # Ø§Ù†ØªÙ‚Ø§Ù„ ÙØ§ÛŒÙ„
                if file_path.exists() and not new_path.exists():
                    file_path.rename(new_path)
                    logger.info(f"ğŸ“ Organized: {file_path.name} -> {new_dir}")
                    return new_path
        
        except Exception as e:
            logger.error(f"Organization error: {e}")
        
        return file_path
    
    async def download_file(self, message, retry_count: int = 0) -> Optional[Dict]:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        max_retries = 3
        
        try:
            if not message.media:
                return None
            
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ÙØ¹Ø§Ù„ÛŒØª Ø§Ù†Ø³Ø§Ù†ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø¯Ø§Ù†Ù„ÙˆØ¯
            await self.simulate_human_activity(message.chat_id)
            await self.human_delay()
            
            # Ø¯Ø±ÛŒØ§ÙØª Ù†Ø§Ù… ÙØ§ÛŒÙ„
            file_name, file_ext = await self.get_file_name(message)
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø§Ù… Ù…Ù†Ø­ØµØ±Ø¨ÙØ±Ø¯
            base_name = Path(file_name).stem
            file_path = self.downloads_dir / file_name
            counter = 1
            
            while file_path.exists():
                new_name = f"{base_name}_{counter}{file_ext}"
                file_path = self.downloads_dir / new_name
                counter += 1
                if counter > 100:  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
                    raise Exception("Too many duplicate filenames")
            
            logger.info(f"ğŸ“¥ Downloading: {file_path.name}")
            
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ timeout
            try:
                await asyncio.wait_for(
                    message.download_media(file=str(file_path)),
                    timeout=300  # 5 minutes timeout
                )
            except asyncio.TimeoutError:
                logger.error("Download timeout")
                if file_path.exists():
                    file_path.unlink()
                raise Exception("Download timeout")
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡
            if not file_path.exists():
                raise Exception("File not found after download")
            
            file_size = file_path.stat().st_size
            max_size = self.settings['safety']['max_file_size_mb'] * 1024 * 1024
            
            if file_size == 0:
                logger.error("Downloaded file is empty")
                file_path.unlink()
                raise Exception("Empty file")
            
            if file_size > max_size:
                logger.error(f"File too large: {file_size:,} > {max_size:,}")
                file_path.unlink()
                raise Exception("File too large")
            
            # ØªØ­Ù„ÛŒÙ„ AI
            ai_analysis = None
            if self.settings['ai']['enable_analysis']:
                caption = message.text or message.message or ""
                ai_analysis = await self.ai_analyzer.analyze_file(
                    file_path, 
                    file_ext.replace('.', '').upper(),
                    caption
                )
                
                # ÙÛŒÙ„ØªØ± Ù…Ø­ØªÙˆØ§
                if not await self.ai_analyzer.filter_content(ai_analysis, self.settings['ai']):
                    logger.info(f"ğŸ—‘ï¸ Content filtered: {file_path.name}")
                    file_path.unlink()
                    return None
                
                # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
                if ai_analysis.file_hash and ai_analysis.file_hash in self.known_hashes:
                    logger.info(f"âš¡ Duplicate detected, skipping: {file_path.name}")
                    file_path.unlink()
                    return None
            
            # Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ ÙØ§ÛŒÙ„
            if ai_analysis:
                file_path = await self.organize_file(file_path, ai_analysis)
            
            # Ø³Ø§Ø®Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„
            file_info = {
                'id': message.id,
                'chat_id': message.chat_id,
                'chat_title': getattr(message.chat, 'title', 'Unknown'),
                'file_name': file_path.name,
                'file_path': str(file_path),
                'file_size': file_size,
                'file_type': file_ext.replace('.', '').upper(),
                'download_time': datetime.now().isoformat(),
                'caption': (message.text or message.message or '')[:1000],
                'message_date': message.date.isoformat() if message.date else None,
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            if self.db.save_file_info(file_info, ai_analysis):
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
                self.download_count_today += 1
                category = ai_analysis.category if ai_analysis else 'unknown'
                self.db.update_statistics(file_size, category)
                
                # Ø§ÙØ²ÙˆØ¯Ù† Ù‡Ø´ Ø¨Ù‡ Ú©Ø´
                if ai_analysis and ai_analysis.file_hash:
                    self.known_hashes.add(ai_analysis.file_hash)
            
            logger.info(f"âœ… Downloaded: {file_path.name} ({file_size:,} bytes)")
            
            # ØªØ§Ø®ÛŒØ± Ù¾Ø³ Ø§Ø² Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÙˆÙÙ‚
            await self.human_delay(4, 10)
            
            return file_info
            
        except Exception as e:
            logger.error(f"Download failed: {e}")
            
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ù†Ø§Ù‚Øµ
            if 'file_path' in locals() and file_path.exists():
                try:
                    file_path.unlink()
                except:
                    pass
            
            # ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯
            if retry_count < max_retries:
                wait_time = 10 * (retry_count + 1)
                logger.info(f"Retrying in {wait_time}s (attempt {retry_count + 1}/{max_retries})")
                await asyncio.sleep(wait_time)
                return await self.download_file(message, retry_count + 1)
            
            return None
    
    async def process_message(self, message):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù… Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø§ÛŒØ· Ø§ÛŒÙ…Ù†
            if not self.is_safe_to_operate():
                return
            
            if not self.can_download_more():
                return
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù…
            if message.media:
                await self.download_file(message)
            
            elif message.text and 't.me/' in message.text:
                await self.process_message_link(message.text)
            
        except Exception as e:
            logger.error(f"Message processing error: {e}")
            await asyncio.sleep(self.settings['safety']['cooldown_after_error'])
    
    async def process_message_link(self, link: str):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„ÛŒÙ†Ú© Ù¾ÛŒØ§Ù…"""
        try:
            parts = link.strip().split('/')
            if len(parts) < 5:
                return
            
            channel_part = parts[-2]
            try:
                message_id = int(parts[-1])
            except ValueError:
                return
            
            message = await self.client.get_messages(channel_part, ids=message_id)
            if message:
                await self.process_message(message)
                
        except Exception as e:
            logger.error(f"Link processing error: {e}")
    
    async def setup_handlers(self):
        """ØªÙ†Ø¸ÛŒÙ… Ù‡Ù†Ø¯Ù„Ø±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        
        @self.client.on(events.NewMessage(incoming=True))
        async def universal_handler(event):
            """Ù‡Ù†Ø¯Ù„Ø± Ø¬Ù‡Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§"""
            try:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
                if not event.message:
                    return
                
                # ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø±Ø³Ø§Ù†Ù‡â€ŒØ§ÛŒ ÛŒØ§ Ø­Ø§ÙˆÛŒ Ù„ÛŒÙ†Ú©
                if event.message.media or ('t.me/' in (event.message.text or '')):
                    await self.process_message(event.message)
                    
            except Exception as e:
                logger.error(f"Handler error: {e}")
        
        @self.client.on(events.NewMessage(pattern=r'^/stats$'))
        async def stats_handler(event):
            """Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
            try:
                today = datetime.now().strftime('%Y-%m-%d')
                
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    
                    # Ø¢Ù…Ø§Ø± Ø§Ù…Ø±ÙˆØ²
                    cursor.execute(
                        "SELECT downloads, total_size FROM statistics WHERE date = ?",
                        (today,)
                    )
                    today_stats = cursor.fetchone()
                    
                    # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
                    cursor.execute("SELECT COUNT(*) as total FROM files")
                    total_files = cursor.fetchone()['total']
                    
                    cursor.execute("SELECT SUM(file_size) as total_size FROM files")
                    total_size = cursor.fetchone()['total_size'] or 0
                    
                    cursor.execute("SELECT COUNT(DISTINCT category) as categories FROM files")
                    categories = cursor.fetchone()['categories']
                
                # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù…
                stats_text = f"""
ğŸ“Š **Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ UserBot**

ğŸ“ˆ **Ø§Ù…Ø±ÙˆØ² ({today}):**
â”œ Ø¯Ø§Ù†Ù„ÙˆØ¯â€ŒÙ‡Ø§: {today_stats['downloads'] if today_stats else 0}
â”” Ø­Ø¬Ù…: {today_stats['total_size']//1024//1024 if today_stats else 0} MB

ğŸ“¦ **Ú©Ù„ÛŒ:**
â”œ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {total_files}
â”œ Ø­Ø¬Ù… Ú©Ù„: {total_size//1024//1024//1024:.1f} GB
â”” Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§: {categories}

âš™ï¸ **ÙˆØ¶Ø¹ÛŒØª:**
â”œ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ù…Ø±ÙˆØ²: {self.download_count_today}/{self.settings['safety']['max_downloads_per_day']}
â”œ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ: {len(self.known_hashes)}
â”” ÙˆØ¶Ø¹ÛŒØª AI: {'ÙØ¹Ø§Ù„ âœ…' if self.ai_analyzer.initialized else 'ØºÛŒØ±ÙØ¹Ø§Ù„ âš ï¸'}
                """
                
                await event.reply(stats_text)
                
            except Exception as e:
                logger.error(f"Stats error: {e}")
                await event.reply("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±")
        
        @self.client.on(events.NewMessage(pattern=r'^/search (.+)$'))
        async def search_handler(event):
            """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§"""
            try:
                keyword = event.pattern_match.group(1)
                
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                    SELECT file_name, category, tags, file_size, download_time 
                    FROM files 
                    WHERE caption LIKE ? OR tags LIKE ? OR category LIKE ?
                    LIMIT 10
                    ''', (f'%{keyword}%', f'%{keyword}%', f'%{keyword}%'))
                    
                    results = cursor.fetchall()
                
                if results:
                    response = f"ğŸ” **Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ '{keyword}':**\n\n"
                    for i, row in enumerate(results, 1):
                        response += f"{i}. **{row['file_name']}**\n"
                        response += f"   ğŸ“ {row['category']} | ğŸ“¦ {row['file_size']//1024} KB\n"
                        response += f"   ğŸ·ï¸ {row['tags'][:50]}...\n"
                        response += f"   ğŸ“… {row['download_time'][:10]}\n\n"
                else:
                    response = f"âŒ Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ '{keyword}' ÛŒØ§ÙØª Ù†Ø´Ø¯."
                
                await event.reply(response)
                
            except Exception as e:
                logger.error(f"Search error: {e}")
        
        @self.client.on(events.NewMessage(pattern=r'^/organize$'))
        async def organize_handler(event):
            """Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ù…Ø¬Ø¯Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§"""
            try:
                await event.reply("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...")
                
                count = 0
                for file_path in self.downloads_dir.rglob('*'):
                    if file_path.is_file():
                        # ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„
                        ai_analysis = await self.ai_analyzer.analyze_file(
                            file_path, 
                            file_path.suffix.replace('.', '').upper(),
                            ""
                        )
                        
                        # Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ
                        new_path = await self.organize_file(file_path, ai_analysis)
                        if new_path != file_path:
                            count += 1
                
                await event.reply(f"âœ… Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯. {count} ÙØ§ÛŒÙ„ Ù…Ø±ØªØ¨ Ø´Ø¯.")
                
            except Exception as e:
                logger.error(f"Organize error: {e}")
                await event.reply("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ")
    
    async def start(self):
        """Ø´Ø±ÙˆØ¹ UserBot Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        logger.info("ğŸš€ Starting Advanced UserBot...")
        
        try:
            # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù„Ø§ÛŒÙ†Øª
            self.client = TelegramClient(
                session=str(self.data_dir / 'advanced_session'),
                api_id=self.api_id,
                api_hash=self.api_hash,
                device_model="Samsung Galaxy S23",
                system_version="Android 14",
                app_version="9.6.1",
                lang_code="fa",
                system_lang_code="fa-IR",
                timeout=60,
                connection_retries=3
            )
            
            # Ø§ØªØµØ§Ù„
            await self.client.start()
            
            me = await self.client.get_me()
            logger.info(f"ğŸ‘¤ Logged in as: {me.first_name} (@{me.username})")
            
            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ AI
            await self.initialize_ai()
            
            # ØªÙ†Ø¸ÛŒÙ… Ù‡Ù†Ø¯Ù„Ø±Ù‡Ø§
            await self.setup_handlers()
            
            # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª
            logger.info("=" * 60)
            logger.info(f"ğŸ“ Downloads Dir: {self.downloads_dir}")
            logger.info(f"ğŸ§  AI Status: {self.ai_analyzer.initialized}")
            logger.info(f"ğŸ—„ï¸ Database: {self.db_path}")
            logger.info(f"â° Working Hours: {self.settings['safety']['working_hours']}")
            logger.info("=" * 60)
            logger.info("âœ… UserBot is running. Commands: /stats, /search, /organize")
            logger.info("ğŸ›‘ Press Ctrl+C to stop")
            
            # Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† Ø±Ø¨Ø§Øª ÙØ¹Ø§Ù„
            await self.client.run_until_disconnected()
            
        except KeyboardInterrupt:
            logger.info("Received interrupt, shutting down...")
        except Exception as e:
            logger.error(f"Fatal error: {e}", exc_info=True)
        finally:
            await self.disconnect()
    
    async def disconnect(self):
        """Ù‚Ø·Ø¹ Ø§Ø±ØªØ¨Ø§Ø· Ø§ÛŒÙ…Ù†"""
        if self.client and self.client.is_connected():
            await self.client.disconnect()
            logger.info("ğŸ”Œ Disconnected from Telegram")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        logger.info("ğŸ“Š Final statistics saved")

# ==================== Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ====================

async def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§"""
    
    config_file = Path(__file__).parent / "config.json"
    
    if not config_file.exists():
        config = {
            "api_id": "YOUR_API_ID_HERE",
            "api_hash": "YOUR_API_HASH_HERE",
            "settings": {
                "safety": {
                    "max_downloads_per_day": 30,
                    "max_file_size_mb": 500
                },
                "ai": {
                    "enable_analysis": True,
                    "filter_nsfw": True
                }
            }
        }
        
        config_file.write_text(json.dumps(config, ensure_ascii=False, indent=2))
        print(f"âš ï¸ ÙØ§ÛŒÙ„ config.json Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.")
        return
    
    config = json.loads(config_file.read_text())
    
    api_id = config.get("api_id")
    api_hash = config.get("api_hash")
    
    if not api_id or not api_hash or api_id == "YOUR_API_ID_HERE":
        print("âŒ Ù„Ø·ÙØ§Ù‹ API credentials Ø±Ø§ Ø¯Ø± config.json ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.")
        return
    
    # Ø§Ø¹Ù…Ø§Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÙØ§Ø±Ø´ÛŒ
    userbot = AdvancedUserBotDownloader(int(api_id), api_hash)
    
    if 'settings' in config:
        # Ø§Ø¯ØºØ§Ù… ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÙØ§Ø±Ø´ÛŒ
        for section, values in config['settings'].items():
            if section in userbot.settings:
                userbot.settings[section].update(values)
    
    await userbot.start()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        logger.critical(f"Application crashed: {e}", exc_info=True)
        print(f"âŒ Application crashed. Check logs for details.")
