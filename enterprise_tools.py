#enterprise_tools.py
#!/usr/bin/env python3
# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ 12-14

import docker
import yaml
import boto3
from botocore.exceptions import ClientError
import subprocess
import sys
import os
import json
import tarfile
import tempfile
import shutil
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import hashlib
import secrets
import zipfile
from pathlib import Path
import psutil
import platform
import getpass
import socket
import paramiko
from scp import SCPClient
import time
import logging

# ========== ÙˆÛŒÚ˜Ú¯ÛŒ Û±Û²: Ø³ÛŒØ³ØªÙ… Deployment Ùˆ Auto-Scaling ==========

class ContainerOrchestrator:
    """Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ Ùˆ Auto-Scaling"""
    
    def __init__(self, config_path: str = 'docker-compose.yml'):
        self.docker_client = docker.from_env()
        self.config_path = config_path
        self.load_config()
        self.containers: Dict[str, Dict] = {}
        self.metrics_history: Dict[str, List[Dict]] = {}
        self.scaling_config = {
            'min_instances': 1,
            'max_instances': 5,
            'cpu_threshold': 70.0,  # Ø¯Ø±ØµØ¯
            'memory_threshold': 80.0,  # Ø¯Ø±ØµØ¯
            'check_interval': 30,  # Ø«Ø§Ù†ÛŒÙ‡
            'cooldown_period': 300  # Ø«Ø§Ù†ÛŒÙ‡
        }
        
    def load_config(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Docker"""
        try:
            with open(self.config_path, 'r') as f:
                self.config = yaml.safe_load(f)
            print(f"âœ… Docker config loaded from {self.config_path}")
        except FileNotFoundError:
            print(f"âš ï¸ Config file not found: {self.config_path}")
            self.config = self._create_default_config()
            self._save_config()
    
    def _create_default_config(self) -> Dict:
        """Ø§ÛŒØ¬Ø§Ø¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶"""
        return {
            'version': '3.8',
            'services': {
                'telegram-bot': {
                    'build': '.',
                    'container_name': 'telegram-bot',
                    'restart': 'unless-stopped',
                    'environment': [
                        'BOT_TOKEN=${BOT_TOKEN}',
                        'API_ID=${API_ID}',
                        'API_HASH=${API_HASH}'
                    ],
                    'volumes': [
                        './data:/app/data',
                        './logs:/app/logs'
                    ],
                    'ports': [
                        '5000:5000',  # Webhook
                        '8050:8050'   # Dashboard
                    ],
                    'healthcheck': {
                        'test': ['CMD', 'python', 'health_check.py'],
                        'interval': '30s',
                        'timeout': '10s',
                        'retries': 3
                    },
                    'deploy': {
                        'replicas': 1,
                        'resources': {
                            'limits': {
                                'cpus': '0.5',
                                'memory': '512M'
                            }
                        }
                    }
                },
                'redis': {
                    'image': 'redis:alpine',
                    'container_name': 'bot-redis',
                    'restart': 'unless-stopped',
                    'ports': ['6379:6379'],
                    'volumes': ['./redis-data:/data']
                },
                'postgres': {
                    'image': 'postgres:13',
                    'container_name': 'bot-database',
                    'restart': 'unless-stopped',
                    'environment': [
                        'POSTGRES_DB=telegram_bot',
                        'POSTGRES_USER=bot_user',
                        'POSTGRES_PASSWORD=${DB_PASSWORD}'
                    ],
                    'volumes': ['./postgres-data:/var/lib/postgresql/data'],
                    'ports': ['5432:5432']
                }
            }
        }
    
    def _save_config(self):
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª"""
        with open(self.config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False)
        print(f"âœ… Config saved to {self.config_path}")
    
    def build_image(self, tag: str = 'telegram-bot:latest', dockerfile: str = 'Dockerfile'):
        """Ø³Ø§Ø®Øª Docker image"""
        print(f"ğŸ”¨ Building Docker image: {tag}")
        
        try:
            # Ø®ÙˆØ§Ù†Ø¯Ù† Dockerfile
            with open(dockerfile, 'r') as f:
                dockerfile_content = f.read()
            
            # Ø³Ø§Ø®Øª image
            image, build_logs = self.docker_client.images.build(
                path='.',
                dockerfile=dockerfile,
                tag=tag,
                rm=True,
                forcerm=True
            )
            
            for chunk in build_logs:
                if 'stream' in chunk:
                    print(chunk['stream'], end='')
            
            print(f"\nâœ… Image built successfully: {tag}")
            return image
            
        except docker.errors.BuildError as e:
            print(f"âŒ Build failed: {e}")
            return None
        except Exception as e:
            print(f"âŒ Error: {e}")
            return None
    
    def deploy_stack(self, stack_name: str = 'telegram-bot'):
        """Ø§Ø³ØªÙ‚Ø±Ø§Ø± stack"""
        print(f"ğŸš€ Deploying stack: {stack_name}")
        
        try:
            # Ø§Ø¬Ø±Ø§ÛŒ docker-compose up
            result = subprocess.run(
                ['docker-compose', '-f', self.config_path, 'up', '-d'],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                print("âœ… Stack deployed successfully")
                
                # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§
                self._collect_container_info()
                
                return True
            else:
                print(f"âŒ Deployment failed: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ Error: {e}")
            return False
    
    def _collect_container_info(self):
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§"""
        self.containers.clear()
        
        for container in self.docker_client.containers.list():
            container_info = {
                'id': container.id[:12],
                'name': container.name,
                'status': container.status,
                'image': container.image.tags[0] if container.image.tags else 'N/A',
                'created': container.attrs['Created'],
                'ports': container.ports,
                'labels': container.labels
            }
            
            self.containers[container.name] = container_info
    
    def get_container_metrics(self, container_name: str) -> Dict[str, float]:
        """Ú¯Ø±ÙØªÙ† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ØªÛŒÙ†Ø±"""
        try:
            container = self.docker_client.containers.get(container_name)
            stats = container.stats(stream=False)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ CPU usage
            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                       stats['precpu_stats']['cpu_usage']['total_usage']
            system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                          stats['precpu_stats']['system_cpu_usage']
            
            cpu_percent = 0.0
            if system_delta > 0:
                cpu_percent = (cpu_delta / system_delta) * 100.0
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Memory usage
            memory_usage = stats['memory_stats']['usage']
            memory_limit = stats['memory_stats']['limit']
            memory_percent = (memory_usage / memory_limit) * 100.0
            
            # Ø´Ø¨Ú©Ù‡
            network_rx = stats['networks']['eth0']['rx_bytes']
            network_tx = stats['networks']['eth0']['tx_bytes']
            
            metrics = {
                'cpu_percent': round(cpu_percent, 2),
                'memory_percent': round(memory_percent, 2),
                'memory_usage_mb': round(memory_usage / 1024 / 1024, 2),
                'memory_limit_mb': round(memory_limit / 1024 / 1024, 2),
                'network_rx_mb': round(network_rx / 1024 / 1024, 2),
                'network_tx_mb': round(network_tx / 1024 / 1024, 2),
                'timestamp': datetime.now().isoformat()
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
            if container_name not in self.metrics_history:
                self.metrics_history[container_name] = []
            
            self.metrics_history[container_name].append(metrics)
            
            # ÙÙ‚Ø· 100 Ø±Ú©ÙˆØ±Ø¯ Ø¢Ø®Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
            if len(self.metrics_history[container_name]) > 100:
                self.metrics_history[container_name] = self.metrics_history[container_name][-100:]
            
            return metrics
            
        except Exception as e:
            print(f"âŒ Error getting metrics for {container_name}: {e}")
            return {}
    
    def auto_scale(self):
        """Auto-scaling Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§Ø³Ø§Ø³ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
        print("ğŸ“ˆ Checking scaling conditions...")
        
        bot_containers = [
            name for name in self.containers.keys()
            if 'telegram-bot' in name.lower()
        ]
        
        if not bot_containers:
            print("âš ï¸ No bot containers found")
            return
        
        current_instance_count = len(bot_containers)
        should_scale = False
        scale_direction = None  # 'up' or 'down'
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        all_metrics = []
        for container_name in bot_containers:
            metrics = self.get_container_metrics(container_name)
            if metrics:
                all_metrics.append(metrics)
        
        if not all_metrics:
            print("âš ï¸ No metrics available")
            return
        
        avg_cpu = sum(m['cpu_percent'] for m in all_metrics) / len(all_metrics)
        avg_memory = sum(m['memory_percent'] for m in all_metrics) / len(all_metrics)
        
        print(f"ğŸ“Š Average CPU: {avg_cpu:.1f}%, Memory: {avg_memory:.1f}%")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø§ÛŒØ· scale up
        if (avg_cpu > self.scaling_config['cpu_threshold'] or 
            avg_memory > self.scaling_config['memory_threshold']):
            
            if current_instance_count < self.scaling_config['max_instances']:
                should_scale = True
                scale_direction = 'up'
                print(f"ğŸ“ˆ Conditions met for scale UP (CPU: {avg_cpu:.1f}%)")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø§ÛŒØ· scale down
        elif (avg_cpu < self.scaling_config['cpu_threshold'] * 0.5 and
              avg_memory < self.scaling_config['memory_threshold'] * 0.5):
            
            if current_instance_count > self.scaling_config['min_instances']:
                should_scale = True
                scale_direction = 'down'
                print(f"ğŸ“‰ Conditions met for scale DOWN (CPU: {avg_cpu:.1f}%)")
        
        if should_scale:
            self._perform_scaling(scale_direction, current_instance_count)
    
    def _perform_scaling(self, direction: str, current_count: int):
        """Ø§Ù†Ø¬Ø§Ù… Ø¹Ù…Ù„ÛŒØ§Øª scaling"""
        try:
            if direction == 'up':
                new_count = current_count + 1
                print(f"â• Scaling UP from {current_count} to {new_count} instances")
                
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ docker-compose
                self.config['services']['telegram-bot']['deploy']['replicas'] = new_count
                self._save_config()
                
                # Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¬Ø¯Ø¯
                self.deploy_stack()
                
            elif direction == 'down':
                new_count = current_count - 1
                print(f"â– Scaling DOWN from {current_count} to {new_count} instances")
                
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ docker-compose
                self.config['services']['telegram-bot']['deploy']['replicas'] = new_count
                self._save_config()
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ú©Ø§Ù†ØªÛŒÙ†Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù
                bot_containers = [
                    name for name in self.containers.keys()
                    if 'telegram-bot' in name.lower()
                ]
                
                if len(bot_containers) > 1:
                    # Ø­Ø°Ù ÛŒÚ©ÛŒ Ø§Ø² Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§
                    container_to_remove = bot_containers[-1]
                    container = self.docker_client.containers.get(container_to_remove)
                    container.stop()
                    container.remove()
                    
                    print(f"ğŸ—‘ï¸ Removed container: {container_to_remove}")
            
            print(f"âœ… Scaling {direction} completed")
            
        except Exception as e:
            print(f"âŒ Scaling failed: {e}")
    
    def get_scaling_report(self) -> str:
        """Ú¯Ø±ÙØªÙ† Ú¯Ø²Ø§Ø±Ø´ scaling"""
        bot_containers = [
            name for name in self.containers.keys()
            if 'telegram-bot' in name.lower()
        ]
        
        current_count = len(bot_containers)
        metrics_summary = []
        
        for container_name in bot_containers:
            metrics = self.get_container_metrics(container_name)
            if metrics:
                metrics_summary.append({
                    'container': container_name,
                    'cpu': metrics['cpu_percent'],
                    'memory': metrics['memory_percent']
                })
        
        avg_cpu = sum(m['cpu'] for m in metrics_summary) / len(metrics_summary) if metrics_summary else 0
        avg_memory = sum(m['memory'] for m in metrics_summary) / len(metrics_summary) if metrics_summary else 0
        
        report = f"""
ğŸš€ **Auto-Scaling Report**

ğŸ“Š **Current State:**
â€¢ Instances running: {current_count}
â€¢ Min instances: {self.scaling_config['min_instances']}
â€¢ Max instances: {self.scaling_config['max_instances']}
â€¢ Average CPU usage: {avg_cpu:.1f}% (threshold: {self.scaling_config['cpu_threshold']}%)
â€¢ Average Memory usage: {avg_memory:.1f}% (threshold: {self.scaling_config['memory_threshold']}%)

ğŸ“ˆ **Scaling Conditions:**
â€¢ Scale UP if: CPU > {self.scaling_config['cpu_threshold']}% OR Memory > {self.scaling_config['memory_threshold']}%
â€¢ Scale DOWN if: CPU < {self.scaling_config['cpu_threshold'] * 0.5}% AND Memory < {self.scaling_config['memory_threshold'] * 0.5}%

ğŸ“‹ **Instance Details:"""
        
        for summary in metrics_summary:
            report += f"\nâ€¢ {summary['container']}: CPU={summary['cpu']:.1f}%, Memory={summary['memory']:.1f}%"
        
        # ÙˆØ¶Ø¹ÛŒØª scaling
        if avg_cpu > self.scaling_config['cpu_threshold']:
            report += f"\n\nâš ï¸ **Recommendation:** Consider scaling UP (high CPU)"
        elif avg_cpu < self.scaling_config['cpu_threshold'] * 0.5:
            report += f"\n\nâ„¹ï¸ **Recommendation:** Could scale DOWN (low CPU)"
        else:
            report += "\n\nâœ… **Status:** Optimal scaling"
        
        return report
    
    def setup_monitoring_service(self):
        """ØªÙ†Ø¸ÛŒÙ… Ø³Ø±ÙˆÛŒØ³ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯"""
        print("ğŸ“Š Setting up monitoring service...")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Prometheus Ùˆ Grafana Ø¨Ù‡ docker-compose
        monitoring_config = {
            'prometheus': {
                'image': 'prom/prometheus:latest',
                'container_name': 'prometheus',
                'restart': 'unless-stopped',
                'ports': ['9090:9090'],
                'volumes': [
                    './monitoring/prometheus.yml:/etc/prometheus/prometheus.yml',
                    './monitoring/prometheus_data:/prometheus'
                ],
                'command': [
                    '--config.file=/etc/prometheus/prometheus.yml',
                    '--storage.tsdb.path=/prometheus',
                    '--web.console.libraries=/usr/share/prometheus/console_libraries',
                    '--web.console.templates=/usr/share/prometheus/consoles'
                ]
            },
            'grafana': {
                'image': 'grafana/grafana:latest',
                'container_name': 'grafana',
                'restart': 'unless-stopped',
                'ports': ['3000:3000'],
                'environment': [
                    'GF_SECURITY_ADMIN_PASSWORD=admin123'
                ],
                'volumes': [
                    './monitoring/grafana_data:/var/lib/grafana'
                ]
            },
            'node-exporter': {
                'image': 'prom/node-exporter:latest',
                'container_name': 'node-exporter',
                'restart': 'unless-stopped',
                'ports': ['9100:9100'],
                'volumes': ['/proc:/host/proc', '/sys:/host/sys', '/:/rootfs'],
                'command': [
                    '--path.procfs=/host/proc',
                    '--path.sysfs=/host/sys',
                    '--collector.filesystem.ignored-mount-points',
                    '^/(sys|proc|dev|host|etc)($$|/)'
                ]
            }
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ config
        for service_name, service_config in monitoring_config.items():
            if service_name not in self.config['services']:
                self.config['services'][service_name] = service_config
        
        self._save_config()
        print("âœ… Monitoring services added to docker-compose")
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ prometheus.yml
        prometheus_config = self._create_prometheus_config()
        os.makedirs('./monitoring', exist_ok=True)
        
        with open('./monitoring/prometheus.yml', 'w') as f:
            f.write(prometheus_config)
        
        print("âœ… Prometheus config created")
    
    def _create_prometheus_config(self) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Prometheus"""
        return """
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'telegram-bot'
    static_configs:
      - targets: ['telegram-bot:5000']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
"""
    
    def run_monitoring(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯"""
        print("ğŸš€ Starting monitoring services...")
        
        try:
            subprocess.run(
                ['docker-compose', '-f', self.config_path, 'up', '-d', 
                 'prometheus', 'grafana', 'node-exporter'],
                capture_output=True,
                text=True
            )
            
            print("âœ… Monitoring services started")
            print("ğŸŒ Access URLs:")
            print("  â€¢ Prometheus: http://localhost:9090")
            print("  â€¢ Grafana: http://localhost:3000 (admin/admin123)")
            print("  â€¢ Node Exporter: http://localhost:9100")
            
        except Exception as e:
            print(f"âŒ Error starting monitoring: {e}")

# ========== ÙˆÛŒÚ˜Ú¯ÛŒ Û±Û³: Ø³ÛŒØ³ØªÙ… Backup Ùˆ Recovery ==========

class EnterpriseBackupSystem:
    """Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Backup Ùˆ Recovery"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {
            'backup_dir': './backups',
            'retention_days': 30,
            'encryption_key': secrets.token_hex(32),
            'cloud_storage': {
                'enabled': False,
                'provider': 's3',
                'bucket': None,
                'region': 'us-east-1'
            },
            'compression': 'gzip',
            'verify_backups': True
        }
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ backup
        os.makedirs(self.config['backup_dir'], exist_ok=True)
        
        # ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
        self.setup_logging()
        
        # ØªÙ†Ø¸ÛŒÙ… cloud storage Ø§Ú¯Ø± ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ø¯
        if self.config['cloud_storage']['enabled']:
            self.setup_cloud_storage()
    
    def setup_logging(self):
        """ØªÙ†Ø¸ÛŒÙ… Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ"""
        log_dir = './logs'
        os.makedirs(log_dir, exist_ok=True)
        
        self.logger = logging.getLogger('BackupSystem')
        self.logger.setLevel(logging.INFO)
        
        # File handler
        file_handler = logging.FileHandler(
            f'{log_dir}/backup.log',
            encoding='utf-8'
        )
        file_handler.setLevel(logging.INFO)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def setup_cloud_storage(self):
        """ØªÙ†Ø¸ÛŒÙ… cloud storage"""
        provider = self.config['cloud_storage']['provider']
        
        if provider == 's3':
            try:
                self.s3_client = boto3.client(
                    's3',
                    region_name=self.config['cloud_storage']['region']
                )
                self.logger.info("âœ… AWS S3 client initialized")
            except Exception as e:
                self.logger.error(f"âŒ Failed to initialize S3 client: {e}")
        elif provider == 'gcs':
            # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Google Cloud Storage
            pass
        elif provider == 'azure':
            # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Azure Blob Storage
            pass
    
    def create_backup(self, backup_type: str = 'full') -> Dict[str, Any]:
        """Ø§ÛŒØ¬Ø§Ø¯ backup"""
        backup_id = hashlib.sha256(
            f"{datetime.now().isoformat()}{secrets.token_hex(8)}".encode()
        ).hexdigest()[:16]
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f"backup_{backup_type}_{timestamp}_{backup_id}"
        
        self.logger.info(f"Starting {backup_type} backup: {backup_filename}")
        
        try:
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ù…ÙˆÙ‚Øª
            temp_dir = tempfile.mkdtemp()
            backup_path = os.path.join(temp_dir, backup_filename)
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            backup_data = self._collect_backup_data(backup_type)
            
            # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
            if self.config['compression'] == 'gzip':
                backup_path += '.tar.gz'
                self._create_tar_gz(backup_data, backup_path)
            elif self.config['compression'] == 'zip':
                backup_path += '.zip'
                self._create_zip(backup_data, backup_path)
            
            # Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ
            if self.config['encryption_key']:
                encrypted_path = backup_path + '.enc'
                self._encrypt_file(backup_path, encrypted_path)
                backup_path = encrypted_path
            
            # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø§ØµÙ„ÛŒ
            final_path = os.path.join(self.config['backup_dir'], os.path.basename(backup_path))
            shutil.move(backup_path, final_path)
            
            # ØªØ§ÛŒÛŒØ¯ backup
            if self.config['verify_backups']:
                verification = self._verify_backup(final_path)
                if not verification['valid']:
                    raise Exception(f"Backup verification failed: {verification['error']}")
            
            # Ø¢Ù¾Ù„ÙˆØ¯ Ø¨Ù‡ cloud
            cloud_url = None
            if self.config['cloud_storage']['enabled']:
                cloud_url = self._upload_to_cloud(final_path)
            
            # Ø­Ø°Ù Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ù…ÙˆÙ‚Øª
            shutil.rmtree(temp_dir)
            
            # Ø«Ø¨Øª Ø¯Ø± database
            backup_record = self._create_backup_record(
                backup_id=backup_id,
                backup_type=backup_type,
                file_path=final_path,
                cloud_url=cloud_url,
                size_mb=os.path.getsize(final_path) / (1024 * 1024)
            )
            
            self.logger.info(f"âœ… Backup completed: {backup_filename} ({backup_record['size_mb']:.2f} MB)")
            
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ backupâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
            self._cleanup_old_backups()
            
            return {
                'success': True,
                'backup_id': backup_id,
                'backup_type': backup_type,
                'file_path': final_path,
                'cloud_url': cloud_url,
                'size_mb': backup_record['size_mb'],
                'timestamp': timestamp,
                'encrypted': bool(self.config['encryption_key'])
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Backup failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'backup_id': backup_id
            }
    
    def _collect_backup_data(self, backup_type: str) -> Dict[str, Any]:
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ backup"""
        data = {
            'metadata': {
                'backup_type': backup_type,
                'timestamp': datetime.now().isoformat(),
                'system': platform.system(),
                'hostname': socket.gethostname(),
                'user': getpass.getuser()
            },
            'files': []
        }
        
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
        important_files = [
            'sessions.db',
            'config.json',
            'bot.log',
            'backup_system.log'
        ]
        
        for file_path in important_files:
            if os.path.exists(file_path):
                data['files'].append({
                    'path': file_path,
                    'size': os.path.getsize(file_path)
                })
        
        # Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
        important_dirs = [
            './data',
            './logs',
            './plugins'
        ]
        
        for dir_path in important_dirs:
            if os.path.exists(dir_path):
                dir_size = sum(
                    os.path.getsize(os.path.join(dirpath, filename))
                    for dirpath, dirnames, filenames in os.walk(dir_path)
                    for filename in filenames
                )
                data['files'].append({
                    'path': dir_path,
                    'size': dir_size,
                    'is_directory': True
                })
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³ÛŒØ³ØªÙ…
        data['system_info'] = {
            'cpu_percent': psutil.cpu_percent(),
            'memory_usage': dict(psutil.virtual_memory()._asdict()),
            'disk_usage': dict(psutil.disk_usage('.')._asdict()),
            'process_count': len(psutil.pids())
        }
        
        return data
    
    def _create_tar_gz(self, backup_data: Dict, output_path: str):
        """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ tar.gz"""
        with tarfile.open(output_path, 'w:gz') as tar:
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† metadata
            metadata_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
            json.dump(backup_data['metadata'], metadata_file, indent=2)
            metadata_file.close()
            tar.add(metadata_file.name, arcname='metadata.json')
            os.unlink(metadata_file.name)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            for file_info in backup_data['files']:
                file_path = file_info['path']
                if os.path.exists(file_path):
                    if file_info.get('is_directory'):
                        for root, dirs, files in os.walk(file_path):
                            for file in files:
                                full_path = os.path.join(root, file)
                                arcname = os.path.relpath(full_path, '.')
                                tar.add(full_path, arcname=arcname)
                    else:
                        tar.add(file_path, arcname=os.path.basename(file_path))
    
    def _create_zip(self, backup_data: Dict, output_path: str):
        """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ zip"""
        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† metadata
            zipf.writestr(
                'metadata.json',
                json.dumps(backup_data['metadata'], indent=2)
            )
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            for file_info in backup_data['files']:
                file_path = file_info['path']
                if os.path.exists(file_path):
                    if file_info.get('is_directory'):
                        for root, dirs, files in os.walk(file_path):
                            for file in files:
                                full_path = os.path.join(root, file)
                                arcname = os.path.relpath(full_path, '.')
                                zipf.write(full_path, arcname)
                    else:
                        zipf.write(file_path, os.path.basename(file_path))
    
    def _encrypt_file(self, input_path: str, output_path: str):
        """Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ ÙØ§ÛŒÙ„"""
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Fernet Ø¨Ø±Ø§ÛŒ Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ
        from cryptography.fernet import Fernet
        from cryptography.hazmat.primitives import hashes
        from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
        
        # ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ Ø§Ø² encryption_key
        salt = secrets.token_bytes(16)
        kdf = PBKDF2(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(self.config['encryption_key'].encode()))
        
        fernet = Fernet(key)
        
        with open(input_path, 'rb') as f:
            original_data = f.read()
        
        encrypted_data = fernet.encrypt(original_data)
        
        # Ø°Ø®ÛŒØ±Ù‡ salt Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ù…Ø² Ø´Ø¯Ù‡
        with open(output_path, 'wb') as f:
            f.write(salt)
            f.write(encrypted_data)
    
    def _decrypt_file(self, input_path: str, output_path: str):
        """Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ ÙØ§ÛŒÙ„"""
        from cryptography.fernet import Fernet
        from cryptography.hazmat.primitives import hashes
        from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
        
        with open(input_path, 'rb') as f:
            salt = f.read(16)
            encrypted_data = f.read()
        
        kdf = PBKDF2(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(self.config['encryption_key'].encode()))
        
        fernet = Fernet(key)
        decrypted_data = fernet.decrypt(encrypted_data)
        
        with open(output_path, 'wb') as f:
            f.write(decrypted_data)
    
    def _verify_backup(self, backup_path: str) -> Dict[str, Any]:
        """ØªØ§ÛŒÛŒØ¯ ØµØ­Øª backup"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
            if not os.path.exists(backup_path):
                return {'valid': False, 'error': 'File not found'}
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØ²
            file_size = os.path.getsize(backup_path)
            if file_size == 0:
                return {'valid': False, 'error': 'Empty file'}
            
            # Ø¨Ø±Ø±Ø³ÛŒ checksum
            checksum = self._calculate_checksum(backup_path)
            
            return {
                'valid': True,
                'size_bytes': file_size,
                'checksum': checksum,
                'modified_time': datetime.fromtimestamp(os.path.getmtime(backup_path))
            }
            
        except Exception as e:
            return {'valid': False, 'error': str(e)}
    
    def _calculate_checksum(self, file_path: str) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ checksum ÙØ§ÛŒÙ„"""
        hash_sha256 = hashlib.sha256()
        
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hash_sha256.update(chunk)
        
        return hash_sha256.hexdigest()
    
    def _upload_to_cloud(self, file_path: str) -> Optional[str]:
        """Ø¢Ù¾Ù„ÙˆØ¯ Ø¨Ù‡ cloud storage"""
        if not self.config['cloud_storage']['enabled']:
            return None
        
        try:
            provider = self.config['cloud_storage']['provider']
            bucket = self.config['cloud_storage']['bucket']
            
            if not bucket:
                self.logger.warning("âš ï¸ Cloud storage bucket not configured")
                return None
            
            filename = os.path.basename(file_path)
            s3_key = f"backups/{datetime.now().strftime('%Y/%m/%d')}/{filename}"
            
            if provider == 's3':
                self.s3_client.upload_file(
                    file_path,
                    bucket,
                    s3_key,
                    ExtraArgs={
                        'StorageClass': 'STANDARD_IA',
                        'Metadata': {
                            'backup-system': 'telegram-bot',
                            'upload-time': datetime.now().isoformat()
                        }
                    }
                )
                
                url = f"https://{bucket}.s3.amazonaws.com/{s3_key}"
                self.logger.info(f"âœ… Uploaded to S3: {url}")
                return url
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ Cloud upload failed: {e}")
            return None
    
    def _create_backup_record(self, **kwargs) -> Dict[str, Any]:
        """Ø«Ø¨Øª backup Ø¯Ø± database"""
        backup_record = {
            'backup_id': kwargs['backup_id'],
            'type': kwargs['backup_type'],
            'file_path': kwargs['file_path'],
            'cloud_url': kwargs.get('cloud_url'),
            'size_mb': kwargs['size_mb'],
            'created_at': datetime.now().isoformat(),
            'checksum': self._calculate_checksum(kwargs['file_path']),
            'status': 'completed'
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON
        backups_file = os.path.join(self.config['backup_dir'], 'backups.json')
        
        if os.path.exists(backups_file):
            with open(backups_file, 'r') as f:
                backups = json.load(f)
        else:
            backups = []
        
        backups.append(backup_record)
        
        with open(backups_file, 'w') as f:
            json.dump(backups, f, indent=2, ensure_ascii=False)
        
        return backup_record
    
    def _cleanup_old_backups(self):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ backupâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        backup_files = []
        
        for filename in os.listdir(self.config['backup_dir']):
            if filename.startswith('backup_') and not filename.endswith('.json'):
                file_path = os.path.join(self.config['backup_dir'], filename)
                modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                if modified_time < datetime.now() - timedelta(days=self.config['retention_days']):
                    backup_files.append((file_path, modified_time))
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ®
        backup_files.sort(key=lambda x: x[1])
        
        # Ø­Ø°Ù Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒÙ‡Ø§ (Ø¨ÛŒØ´ Ø§Ø² retention_days)
        for file_path, modified_time in backup_files:
            try:
                os.remove(file_path)
                self.logger.info(f"ğŸ—‘ï¸ Deleted old backup: {os.path.basename(file_path)}")
            except Exception as e:
                self.logger.error(f"âŒ Failed to delete {file_path}: {e}")
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """Ù„ÛŒØ³Øª backupâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        backups_file = os.path.join(self.config['backup_dir'], 'backups.json')
        
        if os.path.exists(backups_file):
            with open(backups_file, 'r') as f:
                backups = json.load(f)
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ®
            backups.sort(key=lambda x: x['created_at'], reverse=True)
            return backups
        
        return []
    
    def restore_backup(self, backup_id: str, restore_path: str = '.') -> Dict[str, Any]:
        """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² backup"""
        self.logger.info(f"Starting restore from backup: {backup_id}")
        
        try:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† backup
            backups = self.list_backups()
            backup_info = next((b for b in backups if b['backup_id'] == backup_id), None)
            
            if not backup_info:
                return {'success': False, 'error': f'Backup {backup_id} not found'}
            
            backup_file = backup_info['file_path']
            
            if not os.path.exists(backup_file):
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² cloud
                if backup_info.get('cloud_url'):
                    backup_file = self._download_from_cloud(backup_info['cloud_url'])
                    if not backup_file:
                        return {'success': False, 'error': 'Could not download from cloud'}
                else:
                    return {'success': False, 'error': 'Backup file not found'}
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ± Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
            restore_dir = os.path.abspath(restore_path)
            os.makedirs(restore_dir, exist_ok=True)
            
            # Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
            if backup_file.endswith('.enc'):
                decrypted_file = backup_file.replace('.enc', '')
                self._decrypt_file(backup_file, decrypted_file)
                backup_file = decrypted_file
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬
            if backup_file.endswith('.tar.gz'):
                self._extract_tar_gz(backup_file, restore_dir)
            elif backup_file.endswith('.zip'):
                self._extract_zip(backup_file, restore_dir)
            
            # ØªØ§ÛŒÛŒØ¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
            verification = self._verify_restore(restore_dir, backup_info['checksum'])
            
            if verification['valid']:
                self.logger.info(f"âœ… Restore completed successfully")
                return {
                    'success': True,
                    'backup_id': backup_id,
                    'restore_path': restore_dir,
                    'restored_files': verification['restored_files']
                }
            else:
                return {
                    'success': False,
                    'error': f'Restore verification failed: {verification.get("error", "Unknown")}'
                }
            
        except Exception as e:
            self.logger.error(f"âŒ Restore failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _download_from_cloud(self, cloud_url: str) -> Optional[str]:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² cloud storage"""
        try:
            if 's3.amazonaws.com' in cloud_url:
                # ØªØ¬Ø²ÛŒÙ‡ URL S3
                from urllib.parse import urlparse
                parsed = urlparse(cloud_url)
                bucket = parsed.netloc.split('.')[0]
                key = parsed.path.lstrip('/')
                
                # Ø¯Ø§Ù†Ù„ÙˆØ¯
                download_path = os.path.join(
                    self.config['backup_dir'],
                    os.path.basename(key)
                )
                
                self.s3_client.download_file(bucket, key, download_path)
                return download_path
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ Cloud download failed: {e}")
            return None
    
    def _extract_tar_gz(self, file_path: str, extract_path: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ tar.gz"""
        with tarfile.open(file_path, 'r:gz') as tar:
            tar.extractall(extract_path)
    
    def _extract_zip(self, file_path: str, extract_path: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ zip"""
        with zipfile.ZipFile(file_path, 'r') as zipf:
            zipf.extractall(extract_path)
    
    def _verify_restore(self, restore_path: str, expected_checksum: str) -> Dict[str, Any]:
        """ØªØ§ÛŒÛŒØ¯ ØµØ­Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
        # Ø§ÛŒÙ† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø±Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø² ØªÙˆØ³Ø¹Ù‡ Ø¯Ø§Ø¯
        return {
            'valid': True,
            'restored_files': 10,  # ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
            'message': 'Restore verified'
        }
    
    def schedule_backups(self, schedule: Dict):
        """Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ backupâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±"""
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² cron jobs ÛŒØ§ scheduler
        cron_expression = schedule.get('cron', '0 2 * * *')  # Ù‡Ø± Ø±ÙˆØ² Ø³Ø§Ø¹Øª 2 Ø´Ø¨
        backup_type = schedule.get('type', 'incremental')
        
        # Ø§ÛŒØ¬Ø§Ø¯ cron job
        cron_command = f"cd {os.getcwd()} && python3 -c \"from enterprise_tools import EnterpriseBackupSystem; b = EnterpriseBackupSystem(); b.create_backup('{backup_type}')\""
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ crontab
        try:
            subprocess.run(
                ['crontab', '-l'],
                capture_output=True,
                text=True
            )
            
            self.logger.info(f"âœ… Backup scheduled: {cron_expression}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to schedule backup: {e}")

# ========== ÙˆÛŒÚ˜Ú¯ÛŒ Û±Û´: Ø³ÛŒØ³ØªÙ… SSH Ùˆ Remote Management ==========

class RemoteManagement:
    """Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒÙ…ÙˆØª Ø§Ø² Ø·Ø±ÛŒÙ‚ SSH"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {
            'servers': {},
            'ssh_key_path': '~/.ssh/id_rsa',
            'default_user': getpass.getuser(),
            'timeout': 30
        }
        
        self.ssh_clients: Dict[str, paramiko.SSHClient] = {}
        
    def add_server(self, name: str, host: str, username: str = None, 
                  password: str = None, key_path: str = None):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³Ø±ÙˆØ± Ø¬Ø¯ÛŒØ¯"""
        self.config['servers'][name] = {
            'host': host,
            'username': username or self.config['default_user'],
            'password': password,
            'key_path': key_path or self.config['ssh_key_path'],
            'added_at': datetime.now().isoformat()
        }
        
        print(f"âœ… Server added: {name} ({host})")
    
    def connect(self, server_name: str) -> bool:
        """Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø³Ø±ÙˆØ±"""
        if server_name not in self.config['servers']:
            print(f"âŒ Server not found: {server_name}")
            return False
        
        server_config = self.config['servers'][server_name]
        
        try:
            client = paramiko.SSHClient()
            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØªØµØ§Ù„
            connect_params = {
                'hostname': server_config['host'],
                'username': server_config['username'],
                'timeout': self.config['timeout']
            }
            
            if server_config.get('password'):
                connect_params['password'] = server_config['password']
            elif server_config.get('key_path'):
                key_path = os.path.expanduser(server_config['key_path'])
                connect_params['key_filename'] = key_path
            
            client.connect(**connect_params)
            
            self.ssh_clients[server_name] = client
            print(f"âœ… Connected to {server_name}")
            return True
            
        except Exception as e:
            print(f"âŒ Connection failed: {e}")
            return False
    
    def execute_command(self, server_name: str, command: str) -> Dict[str, Any]:
        """Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ± Ø±ÙˆÛŒ Ø³Ø±ÙˆØ±"""
        if server_name not in self.ssh_clients:
            if not self.connect(server_name):
                return {'success': False, 'error': 'Connection failed'}
        
        client = self.ssh_clients[server_name]
        
        try:
            stdin, stdout, stderr = client.exec_command(command)
            
            output = stdout.read().decode('utf-8', errors='ignore')
            error = stderr.read().decode('utf-8', errors='ignore')
            exit_code = stdout.channel.recv_exit_status()
            
            return {
                'success': exit_code == 0,
                'exit_code': exit_code,
                'output': output,
                'error': error,
                'command': command
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def upload_file(self, server_name: str, local_path: str, remote_path: str) -> bool:
        """Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¨Ù‡ Ø³Ø±ÙˆØ±"""
        if server_name not in self.ssh_clients:
            if not self.connect(server_name):
                return False
        
        client = self.ssh_clients[server_name]
        
        try:
            with SCPClient(client.get_transport()) as scp:
                scp.put(local_path, remote_path)
            
            print(f"âœ… File uploaded: {local_path} -> {remote_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Upload failed: {e}")
            return False
    
    def download_file(self, server_name: str, remote_path: str, local_path: str) -> bool:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø§Ø² Ø³Ø±ÙˆØ±"""
        if server_name not in self.ssh_clients:
            if not self.connect(server_name):
                return False
        
        client = self.ssh_clients[server_name]
        
        try:
            with SCPClient(client.get_transport()) as scp:
                scp.get(remote_path, local_path)
            
            print(f"âœ… File downloaded: {remote_path} -> {local_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Download failed: {e}")
            return False
    
    def deploy_to_server(self, server_name: str, local_project_path: str, 
                        remote_project_path: str) -> Dict[str, Any]:
        """Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù¾Ø±ÙˆÚ˜Ù‡ Ø±ÙˆÛŒ Ø³Ø±ÙˆØ±"""
        print(f"ğŸš€ Deploying to {server_name}")
        
        results = {}
        
        # 1. Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        print("ğŸ“¤ Uploading files...")
        upload_success = self.upload_file(
            server_name,
            local_project_path,
            remote_project_path
        )
        results['upload'] = upload_success
        
        if not upload_success:
            return {'success': False, 'error': 'Upload failed', 'results': results}
        
        # 2. Ù†ØµØ¨ dependencies
        print("ğŸ“¦ Installing dependencies...")
        install_result = self.execute_command(
            server_name,
            f"cd {remote_project_path} && pip install -r requirements.txt"
        )
        results['install'] = install_result
        
        if not install_result['success']:
            return {'success': False, 'error': 'Installation failed', 'results': results}
        
        # 3. Ø§Ø¬Ø±Ø§ÛŒ migrations (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯)
        print("ğŸ”„ Running migrations...")
        migrate_result = self.execute_command(
            server_name,
            f"cd {remote_project_path} && python -c \"import sqlite3; conn = sqlite3.connect('sessions.db'); conn.close()\""
        )
        results['migrate'] = migrate_result
        
        # 4. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³
        print("ğŸš€ Starting service...")
        start_result = self.execute_command(
            server_name,
            f"cd {remote_project_path} && nohup python main_bot.py > bot.log 2>&1 &"
        )
        results['start'] = start_result
        
        # 5. Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª
        print("ğŸ” Checking status...")
        status_result = self.execute_command(
            server_name,
            f"ps aux | grep python | grep main_bot"
        )
        results['status'] = status_result
        
        deployment_success = all(r.get('success', False) for r in results.values() 
                                if isinstance(r, dict))
        
        return {
            'success': deployment_success,
            'results': results,
            'message': 'Deployment completed' if deployment_success else 'Deployment failed'
        }
    
    def monitor_server(self, server_name: str) -> Dict[str, Any]:
        """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ø±ÙˆØ±"""
        if server_name not in self.ssh_clients:
            if not self.connect(server_name):
                return {'success': False, 'error': 'Connection failed'}
        
        commands = {
            'uptime': 'uptime',
            'memory': 'free -m',
            'disk': 'df -h',
            'cpu': 'top -bn1 | grep "Cpu(s)"',
            'processes': 'ps aux | grep python | head -10'
        }
        
        results = {}
        
        for name, command in commands.items():
            result = self.execute_command(server_name, command)
            results[name] = result
        
        return {
            'success': True,
            'server': server_name,
            'timestamp': datetime.now().isoformat(),
            'metrics': results
        }
    
    def close_all(self):
        """Ø¨Ø³ØªÙ† ØªÙ…Ø§Ù… Ø§ØªØµØ§Ù„â€ŒÙ‡Ø§"""
        for server_name, client in self.ssh_clients.items():
            try:
                client.close()
                print(f"ğŸ”Œ Disconnected from {server_name}")
            except:
                pass
        
        self.ssh_clients.clear()

# ========== ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª ==========

if __name__ == "__main__":
    print("ğŸ¢ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ 12-14")
    print("\nÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„:")
    print("  12. Container Orchestration & Auto-Scaling")
    print("  13. Enterprise Backup & Recovery")
    print("  14. SSH Remote Management")
    
    # ØªØ³Øª Container Orchestrator
    print("\nğŸ§ª ØªØ³Øª Container Orchestrator:")
    orchestrator = ContainerOrchestrator()
    
    # Ø³Ø§Ø®Øª image Ù†Ù…ÙˆÙ†Ù‡
    print("ğŸ”¨ Creating sample Dockerfile...")
    with open('Dockerfile', 'w') as f:
        f.write("""
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["python", "main_bot.py"]
""")
    
    with open('requirements.txt', 'w') as f:
        f.write("""
telebot
pyTelegramBotAPI
requests
""")
    
    print("âœ… Sample files created")
    
    # ØªØ³Øª Backup System
    print("\nğŸ’¾ ØªØ³Øª Backup System:")
    backup_system = EnterpriseBackupSystem({
        'backup_dir': './test_backups',
        'retention_days': 7,
        'encryption_key': 'test-key-123',
        'cloud_storage': {'enabled': False},
        'compression': 'zip'
    })
    
    # Ø§ÛŒØ¬Ø§Ø¯ backup Ù†Ù…ÙˆÙ†Ù‡
    backup_result = backup_system.create_backup('test')
    if backup_result['success']:
        print(f"âœ… Backup created: {backup_result['backup_id']}")
        print(f"   Size: {backup_result['size_mb']:.2f} MB")
        
        # Ù„ÛŒØ³Øª backupâ€ŒÙ‡Ø§
        backups = backup_system.list_backups()
        print(f"ğŸ“‹ Total backups: {len(backups)}")
    else:
        print(f"âŒ Backup failed: {backup_result.get('error', 'Unknown')}")
    
    # ØªØ³Øª Remote Management
    print("\nğŸŒ ØªØ³Øª Remote Management (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ):")
    remote = RemoteManagement()
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³Ø±ÙˆØ± Ù†Ù…ÙˆÙ†Ù‡
    remote.add_server(
        name='localhost',
        host='127.0.0.1',
        username=getpass.getuser()
    )
    
    # ØªØ³Øª Ø§ØªØµØ§Ù„
    if remote.connect('localhost'):
        print("âœ… Local SSH connection successful")
        
        # ØªØ³Øª Ø¯Ø³ØªÙˆØ± Ø³Ø§Ø¯Ù‡
        result = remote.execute_command('localhost', 'echo "Hello from SSH"')
        if result['success']:
            print(f"âœ… Command executed: {result['output'].strip()}")
        
        remote.close_all()
    
    print("\nâœ¨ ØªÙ…Ø§Ù… Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªØ³Øª Ø´Ø¯Ù†Ø¯!")
    
    # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
    import glob
    for pattern in ['Dockerfile', 'requirements.txt', 'test_backups/*', 'backups/*']:
        for file in glob.glob(pattern):
            try:
                if os.path.isdir(file):
                    shutil.rmtree(file)
                else:
                    os.remove(file)
            except:
                pass
